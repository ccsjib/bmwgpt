{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbfdb9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measyocr\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import easyocr \n",
    "from pdf2image import convert_from_path \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin \n",
    "from langchain_community.document_loaders import WikipediaLoader, WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_PATH = \"bmw_knowledge_db_rag\"\n",
    "DATA_PATH = \"rag_data\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "FOCUS_CARS = [\n",
    "    \"E24\", \"E28\", \"E30\", \"E31\", \"E32\", \"E34\", \n",
    "    \"E36-7\", \"E36-8\", \"E36\", \n",
    "    \"E38\", \"E39\", \"E46\", \n",
    "    \"E52\", \"E53\", \"E83\", \n",
    "    \"Z1\", \"Z3\", \"Z8\"\n",
    "]\n",
    "\n",
    "# --- BROWSER HEADERS (To bypass blocking) ---\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- GPU SAFETY CHECK ---\n",
    "# This decides if we can actually use the P100s or if we must failover to CPU\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Try to actually use the GPU. If arch is wrong, this throws an error.\n",
    "            torch.zeros(1).cuda()\n",
    "            print(\"  ‚úÖ Compatible GPU detected (CUDA).\")\n",
    "            return True # use_gpu = True\n",
    "        except:\n",
    "            print(\"  ‚ö†Ô∏è GPU detected but incompatible (P100 vs PyTorch Version).\")\n",
    "            print(\"  ‚ö†Ô∏è Switching to CPU mode. It will be slower, but it will work.\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "USE_GPU = get_device()\n",
    "\n",
    "# Load class map\n",
    "if not os.path.exists('bmw_class_names.json'):\n",
    "    raise FileNotFoundError(\"Critical error! bmw_class_names.json not found\")\n",
    "\n",
    "with open('bmw_class_names.json', 'r') as f:\n",
    "    class_map = json.load(f)\n",
    "    all_models = list(class_map.values()) \n",
    "\n",
    "# ... (Helper functions remain the same) ...\n",
    "def get_matching_chassis(text):\n",
    "    text = text.lower()\n",
    "    for code in FOCUS_CARS:\n",
    "        if code.lower() in text: return code\n",
    "    return None\n",
    "\n",
    "def is_focus_car(name):\n",
    "    for code in FOCUS_CARS:\n",
    "        if code in name.upper(): return code\n",
    "    return None\n",
    "\n",
    "def scrape_fcp_index():\n",
    "    found = {} \n",
    "    base = \"https://www.fcpeuro.com/blog/tag/bmw?page=\"\n",
    "    print(f\"\\n crawling fcp euro blog...\")\n",
    "    \n",
    "    # Reduced to 5 pages for speed testing\n",
    "    for page in tqdm(range(1, 6), desc=\"scanning fcp\"):\n",
    "        try:\n",
    "            r = requests.get(f\"{base}{page}\", headers=HEADERS, timeout=5)\n",
    "            if r.status_code != 200: continue\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if '/blog/' in href:\n",
    "                    url = href if href.startswith('http') else f\"https://www.fcpeuro.com{href}\"\n",
    "                    code = get_matching_chassis(url)\n",
    "                    if code: found[url] = code\n",
    "        except: continue\n",
    "    return found\n",
    "\n",
    "def scrape_pelican_index():\n",
    "    found = {}\n",
    "    master = \"https://www.pelicanparts.com/bmw/techarticles/tech_main.htm\"\n",
    "    print(f\"\\n crawling pelican parts...\")\n",
    "    try:\n",
    "        r = requests.get(master, headers=HEADERS, timeout=10)\n",
    "        if r.status_code != 200: return {}\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        # 1. Sub-pages\n",
    "        subs = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            code = get_matching_chassis(a.get_text()) or get_matching_chassis(a['href'])\n",
    "            if code and \"tech_main\" in a['href']:\n",
    "                subs.add((urljoin(master, a['href']), code))\n",
    "        \n",
    "        # 2. Articles\n",
    "        for sub_url, code in tqdm(subs, desc=\"scanning sub-pages\"):\n",
    "            try:\n",
    "                sr = requests.get(sub_url, headers=HEADERS, timeout=5)\n",
    "                ss = BeautifulSoup(sr.text, 'html.parser')\n",
    "                for sa in ss.find_all('a', href=True):\n",
    "                    if \"techarticles\" in sa['href'] and sa['href'].endswith(\".htm\"):\n",
    "                        found[urljoin(sub_url, sa['href'])] = code\n",
    "            except: continue\n",
    "    except: pass\n",
    "    return found\n",
    "\n",
    "def build_smart_database():\n",
    "    documents = []\n",
    "    \n",
    "    # PHASE 1: WIKI\n",
    "    print(\"\\n wiki time!\")\n",
    "    for model in tqdm(all_models, desc=\"Wiki\"):\n",
    "        if \"non_bmw\" in model: continue\n",
    "        try:\n",
    "            # Loading just 1 doc to keep it fast\n",
    "            loader = WikipediaLoader(query=model.replace(\"_\", \" \"), load_max_docs=1)\n",
    "            docs = loader.load()\n",
    "            for d in docs:\n",
    "                d.metadata[\"car_model\"] = model\n",
    "                d.metadata[\"source_type\"] = \"General History\"\n",
    "            documents.extend(docs)\n",
    "        except: continue\n",
    "\n",
    "    # PHASE 2: WEB\n",
    "    print(\"\\n web crawlers!\")\n",
    "    fcp = scrape_fcp_index()\n",
    "    pel = scrape_pelican_index()\n",
    "    web_urls = {**fcp, **pel}\n",
    "    \n",
    "    if web_urls:\n",
    "        urls = list(web_urls.keys())\n",
    "        print(f\"  downloading {len(urls)} guides...\")\n",
    "        for i in tqdm(range(0, len(urls), 10), desc=\"downloading\"):\n",
    "            batch = urls[i:i+10]\n",
    "            try:\n",
    "                loader = WebBaseLoader(batch, header_template=HEADERS)\n",
    "                loader.requests_per_second = 2\n",
    "                docs = loader.load()\n",
    "                for d in docs:\n",
    "                    url = d.metadata.get('source', '')\n",
    "                    code = fcp.get(url) or pel.get(url) or \"General\"\n",
    "                    d.metadata[\"car_model\"] = code\n",
    "                    d.metadata[\"source_type\"] = \"Expert Guide\"\n",
    "                    d.page_content = d.page_content.replace(\"\\n\", \" \")\n",
    "                documents.extend(docs)\n",
    "            except: pass\n",
    "\n",
    "    # PHASE 3: PDF (With GPU Check)\n",
    "    print(\"\\n local pdfs (OCR)...\")\n",
    "    print(f\"  OCR Device Mode: {'GPU (Fast)' if USE_GPU else 'CPU (Slow)'}\")\n",
    "    \n",
    "    reader = easyocr.Reader(['en'], gpu=USE_GPU) \n",
    "\n",
    "    for model_name in all_models:\n",
    "        if \"non_bmw\" in model_name: continue\n",
    "        folder = os.path.join(DATA_PATH, model_name)\n",
    "        if os.path.exists(folder) and os.path.isdir(folder):\n",
    "            pdfs = [f for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "            if not pdfs: continue\n",
    "            \n",
    "            code = is_focus_car(model_name) or model_name\n",
    "            print(f\"  processing {len(pdfs)} manuals for {code}...\")\n",
    "            \n",
    "            for pdf in pdfs:\n",
    "                try:\n",
    "                    images = convert_from_path(os.path.join(folder, pdf))\n",
    "                    text = \"\"\n",
    "                    for i, img in enumerate(images):\n",
    "                        res = reader.readtext(np.array(img), detail=0)\n",
    "                        text += f\" [Page {i+1}] \" + \" \".join(res)\n",
    "                    \n",
    "                    documents.append(Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\"car_model\": code, \"source_type\": \"Manual\", \"filename\": pdf}\n",
    "                    ))\n",
    "                    print(f\"    ‚úÖ read {pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå failed {pdf}: {e}\")\n",
    "\n",
    "    # BUILD DB\n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n Building DB with {len(documents)} docs...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(documents)\n",
    "    \n",
    "    if os.path.exists(DB_PATH): shutil.rmtree(DB_PATH)\n",
    "    \n",
    "    # Embeddings also need to know if GPU is safe to use\n",
    "    device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "    emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={'device': device})\n",
    "    \n",
    "    db = Chroma(persist_directory=DB_PATH, embedding_function=emb)\n",
    "    \n",
    "    for i in tqdm(range(0, len(splits), 100), desc=\"indexing\"):\n",
    "        db.add_documents(splits[i:i+100])\n",
    "        \n",
    "    print(f\"\\n üöÄ DONE! Database at {DB_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_smart_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6621d584",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measyocr\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import fitz  # <--- REPLACES pdf2image\n",
    "# from pdf2image import convert_from_path  <--- REMOVED\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.document_loaders import WikipediaLoader, WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_PATH = \"bmw_knowledge_db_rag\"\n",
    "DATA_PATH = \"rag_data\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "FOCUS_CARS = [\n",
    "    \"E24\", \"E28\", \"E30\", \"E31\", \"E32\", \"E34\", \n",
    "    \"E36-7\", \"E36-8\", \"E36\", \n",
    "    \"E38\", \"E39\", \"E46\", \n",
    "    \"E52\", \"E53\", \"E83\", \n",
    "    \"Z1\", \"Z3\", \"Z8\"\n",
    "]\n",
    "\n",
    "# --- BROWSER HEADERS (To bypass blocking) ---\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- GPU SAFETY CHECK ---\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Try to actually use the GPU. If arch is wrong, this throws an error.\n",
    "            torch.zeros(1).cuda()\n",
    "            print(\"  ‚úÖ Compatible GPU detected (CUDA).\")\n",
    "            return True \n",
    "        except:\n",
    "            print(\"  ‚ö†Ô∏è GPU detected but incompatible (P100 vs PyTorch Version).\")\n",
    "            print(\"  ‚ö†Ô∏è Switching to CPU mode. It will be slower, but it will work.\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "USE_GPU = get_device()\n",
    "\n",
    "# Load class map\n",
    "if not os.path.exists('bmw_class_names.json'):\n",
    "    # Just creating a dummy map if missing so code doesn't crash during testing\n",
    "    # You likely have this file, but this is a safety fallback\n",
    "    print(\"Warning: bmw_class_names.json not found, using empty list.\")\n",
    "    all_models = []\n",
    "else:\n",
    "    with open('bmw_class_names.json', 'r') as f:\n",
    "        class_map = json.load(f)\n",
    "        all_models = list(class_map.values()) \n",
    "\n",
    "def get_matching_chassis(text):\n",
    "    text = text.lower()\n",
    "    for code in FOCUS_CARS:\n",
    "        if code.lower() in text: return code\n",
    "    return None\n",
    "\n",
    "def is_focus_car(name):\n",
    "    for code in FOCUS_CARS:\n",
    "        if code in name.upper(): return code\n",
    "    return None\n",
    "\n",
    "def scrape_fcp_index():\n",
    "    found = {} \n",
    "    base = \"https://www.fcpeuro.com/blog/tag/bmw?page=\"\n",
    "    print(f\"\\n crawling fcp euro blog...\")\n",
    "    \n",
    "    for page in tqdm(range(1, 6), desc=\"scanning fcp\"):\n",
    "        try:\n",
    "            r = requests.get(f\"{base}{page}\", headers=HEADERS, timeout=5)\n",
    "            if r.status_code != 200: continue\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if '/blog/' in href:\n",
    "                    url = href if href.startswith('http') else f\"https://www.fcpeuro.com{href}\"\n",
    "                    code = get_matching_chassis(url)\n",
    "                    if code: found[url] = code\n",
    "        except: continue\n",
    "    return found\n",
    "\n",
    "def scrape_pelican_index():\n",
    "    found = {}\n",
    "    master = \"https://www.pelicanparts.com/bmw/techarticles/tech_main.htm\"\n",
    "    print(f\"\\n crawling pelican parts...\")\n",
    "    try:\n",
    "        r = requests.get(master, headers=HEADERS, timeout=10)\n",
    "        if r.status_code != 200: return {}\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        subs = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            code = get_matching_chassis(a.get_text()) or get_matching_chassis(a['href'])\n",
    "            if code and \"tech_main\" in a['href']:\n",
    "                subs.add((urljoin(master, a['href']), code))\n",
    "        \n",
    "        for sub_url, code in tqdm(subs, desc=\"scanning sub-pages\"):\n",
    "            try:\n",
    "                sr = requests.get(sub_url, headers=HEADERS, timeout=5)\n",
    "                ss = BeautifulSoup(sr.text, 'html.parser')\n",
    "                for sa in ss.find_all('a', href=True):\n",
    "                    if \"techarticles\" in sa['href'] and sa['href'].endswith(\".htm\"):\n",
    "                        found[urljoin(sub_url, sa['href'])] = code\n",
    "            except: continue\n",
    "    except: pass\n",
    "    return found\n",
    "\n",
    "def build_smart_database():\n",
    "    documents = []\n",
    "    \n",
    "    # PHASE 1: WIKI\n",
    "    print(\"\\n wiki time!\")\n",
    "    for model in tqdm(all_models, desc=\"Wiki\"):\n",
    "        if \"non_bmw\" in model: continue\n",
    "        try:\n",
    "            loader = WikipediaLoader(query=model.replace(\"_\", \" \"), load_max_docs=1)\n",
    "            docs = loader.load()\n",
    "            for d in docs:\n",
    "                d.metadata[\"car_model\"] = model\n",
    "                d.metadata[\"source_type\"] = \"General History\"\n",
    "            documents.extend(docs)\n",
    "        except: continue\n",
    "\n",
    "    # PHASE 2: WEB\n",
    "    print(\"\\n web crawlers!\")\n",
    "    fcp = scrape_fcp_index()\n",
    "    pel = scrape_pelican_index()\n",
    "    web_urls = {**fcp, **pel}\n",
    "    \n",
    "    if web_urls:\n",
    "        urls = list(web_urls.keys())\n",
    "        print(f\"  downloading {len(urls)} guides...\")\n",
    "        for i in tqdm(range(0, len(urls), 10), desc=\"downloading\"):\n",
    "            batch = urls[i:i+10]\n",
    "            try:\n",
    "                loader = WebBaseLoader(batch, header_template=HEADERS)\n",
    "                loader.requests_per_second = 2\n",
    "                docs = loader.load()\n",
    "                for d in docs:\n",
    "                    url = d.metadata.get('source', '')\n",
    "                    code = fcp.get(url) or pel.get(url) or \"General\"\n",
    "                    d.metadata[\"car_model\"] = code\n",
    "                    d.metadata[\"source_type\"] = \"Expert Guide\"\n",
    "                    d.page_content = d.page_content.replace(\"\\n\", \" \")\n",
    "                documents.extend(docs)\n",
    "            except: pass\n",
    "\n",
    "    # PHASE 3: PDF (With Fitz & GPU Check)\n",
    "    print(\"\\n local pdfs (OCR)...\")\n",
    "    print(f\"  OCR Device Mode: {'GPU (Fast)' if USE_GPU else 'CPU (Slow)'}\")\n",
    "    \n",
    "    reader = easyocr.Reader(['en'], gpu=USE_GPU) \n",
    "\n",
    "    for model_name in all_models:\n",
    "        if \"non_bmw\" in model_name: continue\n",
    "        folder = os.path.join(DATA_PATH, model_name)\n",
    "        if os.path.exists(folder) and os.path.isdir(folder):\n",
    "            pdfs = [f for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "            if not pdfs: continue\n",
    "            \n",
    "            code = is_focus_car(model_name) or model_name\n",
    "            print(f\"  processing {len(pdfs)} manuals for {code}...\")\n",
    "            \n",
    "            for pdf in pdfs:\n",
    "                pdf_path = os.path.join(folder, pdf)\n",
    "                try:\n",
    "                    # --- NEW FITZ LOGIC ---\n",
    "                    doc = fitz.open(pdf_path)\n",
    "                    text = \"\"\n",
    "                    for i, page in enumerate(doc):\n",
    "                        # Render page to image (pixmap)\n",
    "                        pix = page.get_pixmap()\n",
    "                        # Convert to numpy array (H, W, Channels)\n",
    "                        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
    "                        \n",
    "                        # Handle transparency (4 channels -> 3 channels)\n",
    "                        if pix.n == 4:\n",
    "                            img = img[:, :, :3]\n",
    "                            \n",
    "                        # Pass to EasyOCR\n",
    "                        res = reader.readtext(img, detail=0)\n",
    "                        text += f\" [Page {i+1}] \" + \" \".join(res)\n",
    "                    # ----------------------\n",
    "                    \n",
    "                    documents.append(Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\"car_model\": code, \"source_type\": \"Manual\", \"filename\": pdf}\n",
    "                    ))\n",
    "                    print(f\"    ‚úÖ read {pdf}\")\n",
    "                    doc.close() # Close file handle\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå failed {pdf}: {e}\")\n",
    "\n",
    "    # BUILD DB\n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n Building DB with {len(documents)} docs...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(documents)\n",
    "    \n",
    "    if os.path.exists(DB_PATH): shutil.rmtree(DB_PATH)\n",
    "    \n",
    "    # Embeddings setup\n",
    "    device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "    emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={'device': device})\n",
    "    \n",
    "    db = Chroma(persist_directory=DB_PATH, embedding_function=emb)\n",
    "    \n",
    "    for i in tqdm(range(0, len(splits), 100), desc=\"indexing\"):\n",
    "        db.add_documents(splits[i:i+100])\n",
    "        \n",
    "    print(f\"\\n üöÄ DONE! Database at {DB_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_smart_database()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
